= CRAWLER!

This is a simple webcrawler written in Ruby. It's not really meant for anything
hardcore and was written originally to see how connected the pages of a site
are; i.e. can a user reach all of the pages we want them to be able to reach
through links.

== NOTE

Little or none of the rest of this Readme applies right now. Crawler is undergoing
significant refactoring/redesigning that will make it more modular and more
useful. In the meantime the command line switches below do not work and the
crawler *will* move beyond the original host. Better not to use it at all
until a stable release.

=== Usage

./crawler.rb [options] URL

=== Options:

-t, --tags: Comma-separated list of html tags to pick out. Used this to find any extant "<b>" or "<i>" tags.

-x, --exclude: Comma-separated list of strings which, if present in a gathered URL, will cause that URL to be ignored. ".pdf" is a good one.


That's pretty much it. It will not extend itself beyond the initial host,
ignores query strings and fragments. It does not have any current function to
throttle itself and does not handle abnormal HTTP response codes at the moment.

As I say, basic.

Anyway, use at your own risk, don't go crashing people's webservers, don't do
drugs, drink your milk, &c.

=== License

Crawler is covered by the terms of the MIT/X11 license. A copy is included.
